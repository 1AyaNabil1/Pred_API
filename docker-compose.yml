services:
  # MLflow Tracking Server
  mlflow:
    image: ghcr.io/mlflow/mlflow:v2.9.2
    container_name: mlflow-server
    ports:
      - "5001:5000"
    volumes:
      - ./mlartifacts:/mlflow/mlartifacts
      - ./mlruns:/mlflow/mlruns
    environment:
      - MLFLOW_BACKEND_STORE_URI=file:///mlflow/mlruns
      - MLFLOW_DEFAULT_ARTIFACT_ROOT=/mlflow/mlartifacts
    command: >
      mlflow server
      --host 0.0.0.0
      --port 5000
      --backend-store-uri file:///mlflow/mlruns
      --default-artifact-root /mlflow/mlartifacts
    healthcheck:
      test:
        [
          "CMD",
          "python",
          "-c",
          "import urllib.request; urllib.request.urlopen('http://localhost:5000/health')",
        ]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 10s
    networks:
      - pred-api-network

  # Backend API
  backend:
    build:
      context: .
      dockerfile: Dockerfile.backend
    container_name: pred-api-backend
    ports:
      - "8000:8000"
    volumes:
      - ./mlartifacts:/app/mlartifacts
      - ./mlruns:/app/mlruns
      - ./backend:/app
    environment:
      - MLFLOW_BASE_URL=http://mlflow:5000
      - API_HOST=0.0.0.0
      - API_PORT=8000
      - ALLOWED_ORIGINS=["http://localhost:3000","http://localhost:80","http://frontend"]
      - LOG_LEVEL=INFO
      - API_KEY=${API_KEY:-}
      - MLFLOW_AUTH_TOKEN=${MLFLOW_AUTH_TOKEN:-}
    depends_on:
      mlflow:
        condition: service_healthy
    networks:
      - pred-api-network
    restart: unless-stopped

  # Frontend UI
  frontend:
    build:
      context: .
      dockerfile: Dockerfile.frontend
    container_name: pred-api-frontend
    ports:
      - "3000:80"
    environment:
      - REACT_APP_API_URL=http://localhost:8000
    depends_on:
      - backend
    networks:
      - pred-api-network
    restart: unless-stopped

networks:
  pred-api-network:
    driver: bridge

volumes:
  mlflow-data:
  mlartifacts-data:
